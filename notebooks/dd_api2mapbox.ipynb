{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import geojson\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOSSIS\n",
    "dataset = 'glossis'\n",
    "locations_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/locations'\n",
    "nodes_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/nodes'\n",
    "observation_types_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/observationTypes'\n",
    "timeseries_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/timeseries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOFFIS\n",
    "dataset = 'gloffis'\n",
    "locations_url = 'http://tw-151.xtr.deltares.nl:8081/FewsWebServices/rest/digitaledelta/2.0/locations'\n",
    "nodes_url = 'http://tw-151.xtr.deltares.nl:8081/FewsWebServices/rest/digitaledelta/2.0/nodes'\n",
    "observation_types_url = 'http://tw-151.xtr.deltares.nl:8081/FewsWebServices/rest/digitaledelta/2.0/observationtypes'\n",
    "timeseries_url = 'http://tw-151.xtr.deltares.nl:8081/FewsWebServices/rest/digitaledelta/2.0/timeseries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = locations_url\n",
    "print(requests.get(locations_url))\n",
    "resp = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first request and inspect it to get the paging information\n",
    "\n",
    "def paging_iter(url):\n",
    "    \"\"\"iterate over all pages in a url\"\"\"\n",
    "    print(requests.get(url))\n",
    "\n",
    "    resp = requests.get(url).json()\n",
    "    page_size = resp['paging']['maxPageSize']\n",
    "    count = resp['paging']['totalObjectCount']\n",
    "    # Now loop over all pages\n",
    "    n = math.ceil(count / page_size)\n",
    "\n",
    "    # store each json result\n",
    "    # progress please\n",
    "    for i in tqdm.tqdm_notebook(range(n)):\n",
    "        # fire the request. It's not faster in parallel (tested with asyncio), so I keep it serial.\n",
    "        resp = requests.get(url, dict(page=i + 1, pageSize=page_size))\n",
    "        yield resp.json()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e25c6280344554aff26b25e35c0873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "locations_resps = list(paging_iter(locations_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1282c4a89534446380bb00f18bc99d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=931), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect information about the information per location\n",
    "# one request per location\n",
    "timeseries_resps = list(paging_iter(timeseries_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the features\n",
    "features = []\n",
    "for resp in locations_resps:\n",
    "    results = resp['results']\n",
    "    for feature in results:\n",
    "        feature['id'] = feature['properties']['locationId']\n",
    "        features.append(feature)        \n",
    "    \n",
    "features_by_id = {\n",
    "    feature['id']: feature\n",
    "    for feature \n",
    "    in features\n",
    "}\n",
    "\n",
    "timeseries = []\n",
    "for resp in timeseries_resps:\n",
    "    timeseries.extend(resp['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q.obs', 'Q.simulated', 'EPot', 'T', 'P']\n"
     ]
    }
   ],
   "source": [
    "all_quantities = []\n",
    "\n",
    "for series in timeseries:\n",
    "    id = series['location']['properties']['locationId']\n",
    "    feature = features_by_id[id]\n",
    "    quantities = feature['properties'].get('quantities', [])\n",
    "    if dataset == 'glossis':\n",
    "        quantity = series['observationType']['quantity']\n",
    "    elif dataset == 'gloffis':\n",
    "        quantity = series['observationType']['parameterCode']\n",
    "    if quantity not in quantities:\n",
    "        quantities.append(quantity)\n",
    "    if quantity not in all_quantities:\n",
    "        all_quantities.append(quantity)    \n",
    "    \n",
    "print(all_quantities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Booleans for all quantities to each feature\n",
    "for feature in features:\n",
    "    for quantity in all_quantities:\n",
    "        if quantity in feature[\"properties\"][\"quantities\"]:\n",
    "            feature[\"properties\"][quantity] = True\n",
    "        else:\n",
    "            feature[\"properties\"][quantity] = False\n",
    "    features_by_id[feature['id']] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the features\n",
    "collection = geojson.FeatureCollection(features=list(features_by_id.values()))\n",
    "with open('tw-151.geojson', 'w') as f:\n",
    "    geojson.dump(collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tiles until level 12 or something.... with tippecanoe\n",
    "\n",
    "# let's upload it\n",
    "!mapbox upload siggyf.pl-tc012 pl-tc012.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
