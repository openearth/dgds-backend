{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cc070c",
   "metadata": {},
   "source": [
    "# 01 - Storm Surge Dataset JRC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c270d3",
   "metadata": {},
   "source": [
    "This script performs the following tasks:\n",
    "1. [Auth] writes data to Zarr files (cloud-native file format) (AUTh)\n",
    "3. [Deltares] checks and creates a geoJSON from Zarr data (required for the Front-End)\n",
    "2. [Deltares] uploads the Zarr to a Google Cloud Storage (GCS) bucket \n",
    "4. [Deltares] uploads the geoJSON to Mapbox \n",
    "5. [Deltares] update the STAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee2343",
   "metadata": {},
   "source": [
    "TODO: \n",
    "1. make consistent with cf conventions (AUTh?)\n",
    "2. come up with checks (cf conventions) for Zarr file before continueing to create a geoJSON \n",
    "3. [Maarten] bold names in Zarr due to a dimension index, now this is changed\n",
    "4. [Maarten] make imports in generate.py importable\n",
    "5. [Maarten] cannot use dimenions to read variables (see third point..) search a way to connect dimensions and variables, connects to 3rd point. Only works if variable name == dimension name \n",
    "6. [Maarten] Zarr does not add offset automatically..\n",
    "7. Possibly write to GCS bucket first.. implement checks there (as the gcs link is in the STAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81edf748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kras\\Anaconda3\\envs\\bathymetry_env\\lib\\site-packages\\xarray\\backends\\cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n#%load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\\n\\n# imports\\nimport sys\\nimport os\\nimport geojson\\nimport json\\nimport netCDF4 as nc\\nimport pathlib\\nimport platform\\nimport xarray as xr\\nimport pandas as pd\\nimport zarr\\nimport subprocess\\nimport warnings\\nimport numpy as np\\n\\nfrom google.cloud import storage\\nfrom dotenv import dotenv_values\\nfrom itertools import product\\nfrom copy import deepcopy\\nfrom datetime import datetime\\nfrom typing import Any, Dict\\nfrom pystac.extensions.datacube import DatacubeExtension, Dimension, Variable\\nfrom pystac import Asset, CatalogType, Collection, Item, Summaries\\nfrom pystac.layout import BestPracticesLayoutStrategy\\nfrom pystac.stac_io import DefaultStacIO\\nfrom pystac.utils import JoinType, join_path_or_url, safe_urlparse\\nfrom pystac import RelType\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# make root directories importable by appending root to path\\ncwd = pathlib.Path().resolve()\\nsys.path.append(os.path.dirname(cwd))\\n\\n# OS independent path configurations\\nif platform.system() == \\\"Windows\\\":\\n    root = pathlib.Path(\\\"P:/\\\")\\nelse:  # linux or other\\n    root = pathlib.Path(\\\"/p/\\\")\\n\\ncoclico_data_dir = pathlib.Path(root, \\\"11205479-coclico\\\", \\\"data\\\")\";\n",
       "                var nbb_formatted_code = \"# Optional; code formatter, installed as jupyter lab extension\\n#%load_ext lab_black\\n# Optional; code formatter, installed as jupyter notebook extension\\n%load_ext nb_black\\n\\n# imports\\nimport sys\\nimport os\\nimport geojson\\nimport json\\nimport netCDF4 as nc\\nimport pathlib\\nimport platform\\nimport xarray as xr\\nimport pandas as pd\\nimport zarr\\nimport subprocess\\nimport warnings\\nimport numpy as np\\n\\nfrom google.cloud import storage\\nfrom dotenv import dotenv_values\\nfrom itertools import product\\nfrom copy import deepcopy\\nfrom datetime import datetime\\nfrom typing import Any, Dict\\nfrom pystac.extensions.datacube import DatacubeExtension, Dimension, Variable\\nfrom pystac import Asset, CatalogType, Collection, Item, Summaries\\nfrom pystac.layout import BestPracticesLayoutStrategy\\nfrom pystac.stac_io import DefaultStacIO\\nfrom pystac.utils import JoinType, join_path_or_url, safe_urlparse\\nfrom pystac import RelType\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# make root directories importable by appending root to path\\ncwd = pathlib.Path().resolve()\\nsys.path.append(os.path.dirname(cwd))\\n\\n# OS independent path configurations\\nif platform.system() == \\\"Windows\\\":\\n    root = pathlib.Path(\\\"P:/\\\")\\nelse:  # linux or other\\n    root = pathlib.Path(\\\"/p/\\\")\\n\\ncoclico_data_dir = pathlib.Path(root, \\\"11205479-coclico\\\", \\\"data\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black\n",
    "\n",
    "# imports\n",
    "import sys\n",
    "import os\n",
    "import geojson\n",
    "import json\n",
    "import netCDF4 as nc\n",
    "import pathlib\n",
    "import platform\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import subprocess\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import storage\n",
    "from dotenv import dotenv_values\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "from pystac.extensions.datacube import DatacubeExtension, Dimension, Variable\n",
    "from pystac import Asset, CatalogType, Collection, Item, Summaries\n",
    "from pystac.layout import BestPracticesLayoutStrategy\n",
    "from pystac.stac_io import DefaultStacIO\n",
    "from pystac.utils import JoinType, join_path_or_url, safe_urlparse\n",
    "from pystac import RelType\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# make root directories importable by appending root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "# OS independent path configurations\n",
    "if platform.system() == \"Windows\":\n",
    "    root = pathlib.Path(\"P:/\")\n",
    "else:  # linux or other\n",
    "    root = pathlib.Path(\"/p/\")\n",
    "\n",
    "coclico_data_dir = pathlib.Path(root, \"11205479-coclico\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b081e572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# paths to the dataset, manual input\\ndataset_dir = coclico_data_dir.joinpath(\\\"01_storm_surge_jrc\\\")\\ndataset_historical_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_Historical.nc\\\")\\ndataset_rcp45_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_RCP45.nc\\\")\\ndataset_rcp85_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_RCP85.nc\\\")\\ndataset_out_file = \\\"CoastAlRisk_Europe_EESSL\\\"\\n\\n# GCS and mapbox private access keys\\nGCS_token = coclico_data_dir.joinpath(\\n    \\\"google_credentials.json\\\"\\n)  # path name (including json file name)\\nconfig = dotenv_values(\\\".env\\\")\\nmapbox_token = config[\\\"MAPBOX_TOKEN\\\"]  # mapbox private key\\n\\n# STAC custom functions\\nlocal_STAC = r\\\"../../coclicodata\\\"  # path to local GitHub STAC clone\\nsys.path.insert(-1, local_STAC)\\nimport generate\";\n",
       "                var nbb_formatted_code = \"# paths to the dataset, manual input\\ndataset_dir = coclico_data_dir.joinpath(\\\"01_storm_surge_jrc\\\")\\ndataset_historical_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_Historical.nc\\\")\\ndataset_rcp45_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_RCP45.nc\\\")\\ndataset_rcp85_path = dataset_dir.joinpath(\\\"CoastAlRisk_Europe_EESSL_RCP85.nc\\\")\\ndataset_out_file = \\\"CoastAlRisk_Europe_EESSL\\\"\\n\\n# GCS and mapbox private access keys\\nGCS_token = coclico_data_dir.joinpath(\\n    \\\"google_credentials.json\\\"\\n)  # path name (including json file name)\\nconfig = dotenv_values(\\\".env\\\")\\nmapbox_token = config[\\\"MAPBOX_TOKEN\\\"]  # mapbox private key\\n\\n# STAC custom functions\\nlocal_STAC = r\\\"../../coclicodata\\\"  # path to local GitHub STAC clone\\nsys.path.insert(-1, local_STAC)\\nimport generate\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# paths to the dataset, manual input\n",
    "dataset_dir = coclico_data_dir.joinpath(\"01_storm_surge_jrc\")\n",
    "dataset_historical_path = dataset_dir.joinpath(\"CoastAlRisk_Europe_EESSL_Historical.nc\")\n",
    "dataset_rcp45_path = dataset_dir.joinpath(\"CoastAlRisk_Europe_EESSL_RCP45.nc\")\n",
    "dataset_rcp85_path = dataset_dir.joinpath(\"CoastAlRisk_Europe_EESSL_RCP85.nc\")\n",
    "dataset_out_file = \"CoastAlRisk_Europe_EESSL\"\n",
    "\n",
    "# GCS and mapbox private access keys\n",
    "GCS_token = coclico_data_dir.joinpath(\n",
    "    \"google_credentials.json\"\n",
    ")  # path name (including json file name)\n",
    "config = dotenv_values(\".env\")\n",
    "mapbox_token = config[\"MAPBOX_TOKEN\"]  # mapbox private key\n",
    "\n",
    "# STAC custom functions\n",
    "local_STAC = r\"../../coclicodata\"  # path to local GitHub STAC clone\n",
    "sys.path.insert(-1, local_STAC)\n",
    "import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde6f94",
   "metadata": {},
   "source": [
    "# 1. write data to Zarr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3358af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# open datasets\\ndataset_historical = xr.open_dataset(dataset_historical_path)\\ndataset_45rcp = xr.open_dataset(dataset_rcp45_path)\\ndataset_85rcp = xr.open_dataset(dataset_rcp85_path)\\n\\n# check original dataset\\n# dataset_historical\";\n",
       "                var nbb_formatted_code = \"# open datasets\\ndataset_historical = xr.open_dataset(dataset_historical_path)\\ndataset_45rcp = xr.open_dataset(dataset_rcp45_path)\\ndataset_85rcp = xr.open_dataset(dataset_rcp85_path)\\n\\n# check original dataset\\n# dataset_historical\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open datasets\n",
    "dataset_historical = xr.open_dataset(dataset_historical_path)\n",
    "dataset_45rcp = xr.open_dataset(dataset_rcp45_path)\n",
    "dataset_85rcp = xr.open_dataset(dataset_rcp85_path)\n",
    "\n",
    "# check original dataset\n",
    "# dataset_historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7185660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# rename dimension names\\ndataset_historical = dataset_historical.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\ndataset_45rcp = dataset_45rcp.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\ndataset_85rcp = dataset_85rcp.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\n\\n# rename variables, if necessary\\n# dataset_historical = dataset_historical.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n# dataset_45rcp = dataset_45rcp.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n# dataset_85rcp = dataset_85rcp.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n\\n# set some data variables to coordinates to avoid duplication of dimensions in later stage\\ndataset_historical = dataset_historical.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\\ndataset_45rcp = dataset_45rcp.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\\ndataset_85rcp = dataset_85rcp.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\";\n",
       "                var nbb_formatted_code = \"# rename dimension names\\ndataset_historical = dataset_historical.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\ndataset_45rcp = dataset_45rcp.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\ndataset_85rcp = dataset_85rcp.rename_dims({\\\"row\\\": \\\"stations\\\", \\\"col\\\": \\\"rp\\\"})\\n\\n# rename variables, if necessary\\n# dataset_historical = dataset_historical.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n# dataset_45rcp = dataset_45rcp.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n# dataset_85rcp = dataset_85rcp.rename_vars({\\\"RP\\\": \\\"rp\\\"})\\n\\n# set some data variables to coordinates to avoid duplication of dimensions in later stage\\ndataset_historical = dataset_historical.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\\ndataset_45rcp = dataset_45rcp.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\\ndataset_85rcp = dataset_85rcp.set_coords([\\\"longitude\\\", \\\"latitude\\\", \\\"RP\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rename dimension names\n",
    "dataset_historical = dataset_historical.rename_dims({\"row\": \"stations\", \"col\": \"rp\"})\n",
    "dataset_45rcp = dataset_45rcp.rename_dims({\"row\": \"stations\", \"col\": \"rp\"})\n",
    "dataset_85rcp = dataset_85rcp.rename_dims({\"row\": \"stations\", \"col\": \"rp\"})\n",
    "\n",
    "# rename variables, if necessary\n",
    "# dataset_historical = dataset_historical.rename_vars({\"RP\": \"rp\"})\n",
    "# dataset_45rcp = dataset_45rcp.rename_vars({\"RP\": \"rp\"})\n",
    "# dataset_85rcp = dataset_85rcp.rename_vars({\"RP\": \"rp\"})\n",
    "\n",
    "# set some data variables to coordinates to avoid duplication of dimensions in later stage\n",
    "dataset_historical = dataset_historical.set_coords([\"longitude\", \"latitude\", \"RP\"])\n",
    "dataset_45rcp = dataset_45rcp.set_coords([\"longitude\", \"latitude\", \"RP\"])\n",
    "dataset_85rcp = dataset_85rcp.set_coords([\"longitude\", \"latitude\", \"RP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df43b59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\\ndataset = xr.concat(\\n    [dataset_historical, dataset_45rcp, dataset_85rcp],\\n    pd.Index([\\\"historical\\\", \\\"rcp45\\\", \\\"rcp85\\\"], name=\\\"scenario\\\"),\\n)\\n\\n# rename dimension names for the concatenated variable\\ndataset = dataset.rename_dims({\\\"scenario\\\": \\\"scenarios\\\"})\";\n",
       "                var nbb_formatted_code = \"# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\\ndataset = xr.concat(\\n    [dataset_historical, dataset_45rcp, dataset_85rcp],\\n    pd.Index([\\\"historical\\\", \\\"rcp45\\\", \\\"rcp85\\\"], name=\\\"scenario\\\"),\\n)\\n\\n# rename dimension names for the concatenated variable\\ndataset = dataset.rename_dims({\\\"scenario\\\": \\\"scenarios\\\"})\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\n",
    "dataset = xr.concat(\n",
    "    [dataset_historical, dataset_45rcp, dataset_85rcp],\n",
    "    pd.Index([\"historical\", \"rcp45\", \"rcp85\"], name=\"scenario\"),\n",
    ")\n",
    "\n",
    "# rename dimension names for the concatenated variable\n",
    "dataset = dataset.rename_dims({\"scenario\": \"scenarios\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b92a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# re-order shape of the data variables\\ndataset = dataset.transpose(\\\"scenarios\\\", \\\"stations\\\", \\\"rp\\\")\";\n",
       "                var nbb_formatted_code = \"# re-order shape of the data variables\\ndataset = dataset.transpose(\\\"scenarios\\\", \\\"stations\\\", \\\"rp\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# re-order shape of the data variables\n",
    "dataset = dataset.transpose(\"scenarios\", \"stations\", \"rp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae43275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (stations: 2242, scenarios: 3, rp: 8)\n",
       "Coordinates:\n",
       "    longitude  (stations) float64 -0.1 -0.1 -0.1 -0.1 -0.3 ... 9.9 9.9 9.9 9.9\n",
       "    latitude   (stations) float64 36.1 39.3 49.7 54.3 ... 57.7 58.7 64.5 64.7\n",
       "    RP         (rp) float32 5.0 10.0 20.0 50.0 100.0 200.0 500.0 1e+03\n",
       "    scenario   (scenarios) object &#x27;historical&#x27; &#x27;rcp45&#x27; &#x27;rcp85&#x27;\n",
       "Dimensions without coordinates: stations, scenarios, rp\n",
       "Data variables:\n",
       "    ssl        (scenarios, stations, rp) float64 1.024 1.051 ... 2.712 2.805\n",
       "Attributes:\n",
       "    title:            European extreme storm surge level\n",
       "    Institution:      Joint European Research Center, Institute of Environmen...\n",
       "    Project Name:     Prototype of a first Global Integrated Coastal Impact-b...\n",
       "    Project Acronym:  CoastAlRisk\n",
       "    reference:        Vousdoukas MI, Voukouvalas E, Annunziato A, Giardino A,...\n",
       "    email:            michail.vousdoukas@ec.europa.eu\n",
       "    version:          1.0\n",
       "    terms_for_use:    European Union, 1995-2015.\\nReuse is authorised, provid...\n",
       "    disclaimer:       Unless the following would not be permitted or valid un...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fbd7af31-4d6f-4250-a02e-2ae87e545397' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fbd7af31-4d6f-4250-a02e-2ae87e545397' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>stations</span>: 2242</li><li><span>scenarios</span>: 3</li><li><span>rp</span>: 8</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-de16e7fe-9601-4cbc-a4d9-891f9c163e84' class='xr-section-summary-in' type='checkbox'  checked><label for='section-de16e7fe-9601-4cbc-a4d9-891f9c163e84' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(stations)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.1 -0.1 -0.1 -0.1 ... 9.9 9.9 9.9</div><input id='attrs-c8862af4-018b-4a75-a0f1-579056fa53d9' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c8862af4-018b-4a75-a0f1-579056fa53d9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a8662e73-f9d2-4c2b-86c9-8eb379dca9ad' class='xr-var-data-in' type='checkbox'><label for='data-a8662e73-f9d2-4c2b-86c9-8eb379dca9ad' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd></dl></div><div class='xr-var-data'><pre>array([-0.1, -0.1, -0.1, ...,  9.9,  9.9,  9.9])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(stations)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>36.1 39.3 49.7 ... 58.7 64.5 64.7</div><input id='attrs-beb71b1f-aea5-4cb5-bdc5-5c6a945b6448' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-beb71b1f-aea5-4cb5-bdc5-5c6a945b6448' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-87be8c25-8297-49cd-9f26-a932d77cd394' class='xr-var-data-in' type='checkbox'><label for='data-87be8c25-8297-49cd-9f26-a932d77cd394' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd></dl></div><div class='xr-var-data'><pre>array([36.1, 39.3, 49.7, ..., 58.7, 64.5, 64.7])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>RP</span></div><div class='xr-var-dims'>(rp)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>5.0 10.0 20.0 ... 200.0 500.0 1e+03</div><input id='attrs-ed6f8050-5b7a-428c-b04c-90f654ada519' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ed6f8050-5b7a-428c-b04c-90f654ada519' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3c947e6d-76d5-4642-8b98-176d7aa4f301' class='xr-var-data-in' type='checkbox'><label for='data-3c947e6d-76d5-4642-8b98-176d7aa4f301' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>return period</dd><dt><span>units :</span></dt><dd>yr</dd><dt><span>Contents :</span></dt><dd>The RPs have been estimated following the Peak Over Threshold Method (see reference below)</dd><dt><span>Starting date :</span></dt><dd>01-Dec-1969</dd><dt><span>End date :</span></dt><dd>30-Nov-2004 21:00:00</dd></dl></div><div class='xr-var-data'><pre>array([   5.,   10.,   20.,   50.,  100.,  200.,  500., 1000.],\n",
       "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>scenario</span></div><div class='xr-var-dims'>(scenarios)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;historical&#x27; &#x27;rcp45&#x27; &#x27;rcp85&#x27;</div><input id='attrs-728ecf60-f71e-4ddc-b835-d5100b8227e8' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-728ecf60-f71e-4ddc-b835-d5100b8227e8' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-025e54c0-e6db-44d8-b114-5abd584b22ac' class='xr-var-data-in' type='checkbox'><label for='data-025e54c0-e6db-44d8-b114-5abd584b22ac' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;historical&#x27;, &#x27;rcp45&#x27;, &#x27;rcp85&#x27;], dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-48b17bc2-286d-46b8-bd75-c37dff45a38f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-48b17bc2-286d-46b8-bd75-c37dff45a38f' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>ssl</span></div><div class='xr-var-dims'>(scenarios, stations, rp)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>1.024 1.051 1.078 ... 2.712 2.805</div><input id='attrs-8c7d828d-7f0b-4226-8eb2-19cbf9ad24a9' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8c7d828d-7f0b-4226-8eb2-19cbf9ad24a9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ac248d51-dddd-4b3a-9998-9c038c720330' class='xr-var-data-in' type='checkbox'><label for='data-ac248d51-dddd-4b3a-9998-9c038c720330' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>storm surge level</dd><dt><span>units :</span></dt><dd>m</dd></dl></div><div class='xr-var-data'><pre>array([[[1.02407, 1.0509 , 1.0778 , ..., 1.16939, 1.20762, 1.23753],\n",
       "        [1.24336, 1.28508, 1.32805, ..., 1.4857 , 1.55811, 1.61812],\n",
       "        [1.95   , 2.08892, 2.20901, ..., 2.51257, 2.60544, 2.66859],\n",
       "        ...,\n",
       "        [2.05624, 2.20418, 2.33512, ..., 2.67975, 2.78853, 2.86284],\n",
       "        [1.86446, 2.02245, 2.16554, ..., 2.55166, 2.67389, 2.7566 ],\n",
       "        [1.87579, 2.02328, 2.1549 , ..., 2.49943, 2.60478, 2.67487]],\n",
       "\n",
       "       [[1.03165, 1.05851, 1.08478, ..., 1.16899, 1.20167, 1.22622],\n",
       "        [1.23326, 1.27602, 1.32215, ..., 1.50603, 1.59589, 1.67189],\n",
       "        [1.97503, 2.12185, 2.25063, ..., 2.58303, 2.68587, 2.75567],\n",
       "        ...,\n",
       "        [2.11645, 2.25912, 2.38164, ..., 2.68137, 2.7671 , 2.82265],\n",
       "        [1.91546, 2.07808, 2.22304, ..., 2.60134, 2.71649, 2.79291],\n",
       "        [1.9071 , 2.05876, 2.19542, ..., 2.56052, 2.67487, 2.75187]],\n",
       "\n",
       "       [[1.03412, 1.06436, 1.09628, ..., 1.21896, 1.27761, 1.32699],\n",
       "        [1.22974, 1.2753 , 1.32831, ..., 1.5973 , 1.77785, 1.96619],\n",
       "        [1.985  , 2.12493, 2.24118, ..., 2.50843, 2.57984, 2.62478],\n",
       "        ...,\n",
       "        [2.10311, 2.24885, 2.37682, ..., 2.70514, 2.80472, 2.87116],\n",
       "        [1.92381, 2.08563, 2.23729, ..., 2.67873, 2.83175, 2.94024],\n",
       "        [1.89731, 2.04812, 2.18738, ..., 2.58059, 2.7125 , 2.80462]]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7b0f5128-a736-4ff3-a555-b56817e8c60e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7b0f5128-a736-4ff3-a555-b56817e8c60e' class='xr-section-summary' >Attributes: <span>(9)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>title :</span></dt><dd>European extreme storm surge level</dd><dt><span>Institution :</span></dt><dd>Joint European Research Center, Institute of Environment and Sustainability, Via Enrico Fermi 2749, I-21027-Ispra</dd><dt><span>Project Name :</span></dt><dd>Prototype of a first Global Integrated Coastal Impact-based Flood Alert and Risk Assessment Tool</dd><dt><span>Project Acronym :</span></dt><dd>CoastAlRisk</dd><dt><span>reference :</span></dt><dd>Vousdoukas MI, Voukouvalas E, Annunziato A, Giardino A, Feyen L. Projections of extreme storm surge levels along Europe. Clim Dyn. February 2016. doi:10.1007/s00382-016-3019-5</dd><dt><span>email :</span></dt><dd>michail.vousdoukas@ec.europa.eu</dd><dt><span>version :</span></dt><dd>1.0</dd><dt><span>terms_for_use :</span></dt><dd>European Union, 1995-2015.\n",
       "Reuse is authorised, provided the source is acknowledged. The reuse policy of the European Commission is implemented by a Decision of 12 December 2011.</dd><dt><span>disclaimer :</span></dt><dd>Unless the following would not be permitted or valid under applicable law, the following applies to the data/information provided by the JRC:\n",
       "\n",
       "1. The JRC data are provided &quot;as is&quot; and &quot;as available&quot; without warranty of any kind, either express or implied, including, but not limited to, any implied warranty against infringement of third parties&#x27; property rights, or merchantability, integration, absence of latent or other defects, satisfactory quality and fitness for a particular purpose. The JRC data do not constitute professional or legal advice (if you need specific advice, you should always consult a suitably qualified professional).\n",
       "2. The JRC has no obligation to provide technical support or remedies for the data. The JRC does not represent or warrant that the data will be error free or uninterrupted, or that all non-conformities can or will be corrected, or that any data are accurate or complete, or that they are of a satisfactory technical or scientific quality.\n",
       "3. The JRC or as the case may be the European Commission shall not be held liable for any direct or indirect, incidental, consequential or other damages, including but not limited to the loss of data, loss of profits, or any other financial loss arising from the use of the JRC data, or inability to use them, even if the JRC is notified of the possibility of such damages.</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (stations: 2242, scenarios: 3, rp: 8)\n",
       "Coordinates:\n",
       "    longitude  (stations) float64 -0.1 -0.1 -0.1 -0.1 -0.3 ... 9.9 9.9 9.9 9.9\n",
       "    latitude   (stations) float64 36.1 39.3 49.7 54.3 ... 57.7 58.7 64.5 64.7\n",
       "    RP         (rp) float32 5.0 10.0 20.0 50.0 100.0 200.0 500.0 1e+03\n",
       "    scenario   (scenarios) object 'historical' 'rcp45' 'rcp85'\n",
       "Dimensions without coordinates: stations, scenarios, rp\n",
       "Data variables:\n",
       "    ssl        (scenarios, stations, rp) float64 1.024 1.051 ... 2.712 2.805\n",
       "Attributes:\n",
       "    title:            European extreme storm surge level\n",
       "    Institution:      Joint European Research Center, Institute of Environmen...\n",
       "    Project Name:     Prototype of a first Global Integrated Coastal Impact-b...\n",
       "    Project Acronym:  CoastAlRisk\n",
       "    reference:        Vousdoukas MI, Voukouvalas E, Annunziato A, Giardino A,...\n",
       "    email:            michail.vousdoukas@ec.europa.eu\n",
       "    version:          1.0\n",
       "    terms_for_use:    European Union, 1995-2015.\\nReuse is authorised, provid...\n",
       "    disclaimer:       Unless the following would not be permitted or valid un..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# check the xarray dataset\\ndataset\";\n",
       "                var nbb_formatted_code = \"# check the xarray dataset\\ndataset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the xarray dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a88562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x1f69aa369e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export to zarr in write mode (to overwrite if exists)\\ndataset.to_zarr(dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file), mode=\\\"w\\\")\";\n",
       "                var nbb_formatted_code = \"# export to zarr in write mode (to overwrite if exists)\\ndataset.to_zarr(dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file), mode=\\\"w\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export to zarr in write mode (to overwrite if exists)\n",
    "dataset.to_zarr(dataset_dir.joinpath(\"%s.zarr\" % dataset_out_file), mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21989f",
   "metadata": {},
   "source": [
    "# 2. check and create geoJSON from Zarr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e6a0735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# load (locally or cloud) stored Zarr\\n# zarr_fn = str(dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file))  # local file path\\nzarr_fn = (\\n    r\\\"gcs://dgds-data-public/coclico/CoastAlRisk_Europe_EESSL.zarr\\\"  # cloud file path\\n)\\nnddata = xr.open_zarr(zarr_fn)\\n\\n# specify input variables\\nmapbox_url = \\\"https://global-data-viewer\\\"\\ntemplate = \\\"deltares-coclico-ssl\\\"\\nvariable = \\\"ssl\\\"  # note, variable is set as different variables might not have the same dimensions\\ndatasetid = f\\\"{variable}-mapbox\\\"  # f\\\"deltares-coclico-{variable}\\\"\";\n",
       "                var nbb_formatted_code = \"# load (locally or cloud) stored Zarr\\n# zarr_fn = str(dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file))  # local file path\\nzarr_fn = (\\n    r\\\"gcs://dgds-data-public/coclico/CoastAlRisk_Europe_EESSL.zarr\\\"  # cloud file path\\n)\\nnddata = xr.open_zarr(zarr_fn)\\n\\n# specify input variables\\nmapbox_url = \\\"https://global-data-viewer\\\"\\ntemplate = \\\"deltares-coclico-ssl\\\"\\nvariable = \\\"ssl\\\"  # note, variable is set as different variables might not have the same dimensions\\ndatasetid = f\\\"{variable}-mapbox\\\"  # f\\\"deltares-coclico-{variable}\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load (locally or cloud) stored Zarr\n",
    "# zarr_fn = str(dataset_dir.joinpath(\"%s.zarr\" % dataset_out_file))  # local file path\n",
    "zarr_fn = (\n",
    "    r\"gcs://dgds-data-public/coclico/CoastAlRisk_Europe_EESSL.zarr\"  # cloud file path\n",
    ")\n",
    "nddata = xr.open_zarr(zarr_fn)\n",
    "\n",
    "# specify input variables\n",
    "mapbox_url = \"https://global-data-viewer\"\n",
    "template = \"deltares-coclico-ssl\"\n",
    "variable = \"ssl\"  # note, variable is set as different variables might not have the same dimensions\n",
    "datasetid = f\"{variable}-mapbox\"  # f\"deltares-coclico-{variable}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8d80e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# check geojson\\n\\n# TODO Come up with checks\";\n",
       "                var nbb_formatted_code = \"# check geojson\\n\\n# TODO Come up with checks\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check geojson\n",
    "\n",
    "# TODO Come up with checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559e872d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# write geoJSON\\n\\n# automated variable retrieval (without hidden files)\\nvariables = list(nddata.variables)\\n\\n# autmated dimension retrieval\\ndimensions = list(nddata[\\\"%s\\\" % (variable)].dims)\\n\\n# write data to flattened GeoJSON file - Mapbox styling uses this (aligned with STAC)\\ncube_dimensions = {}\\nfor dimension in dimensions:  # loop over dimensions of variable\\n\\n    for var in variables:  # identify variable with dimension\\n        var_dim = nddata[\\\"%s\\\" % var].dims\\n        if (\\n            len(var_dim) == 1 and var_dim[0] == dimension and dimension != \\\"stations\\\"\\n        ):  # only take the variables in the coordinates with a single dimension, where dimension is equal to the\\n            # variable dimensions and where it is not equal to stations (assumed present and independent)\\n\\n            dim = nddata[var]\\n\\n            # Only applicable in proper CF convention zarr\\n            #             if dimension == \\\"stations\\\": # TODO Remove because this is abundant due to if statement\\n            #                 dimdict = {\\n            #                     \\\"type\\\": \\\"stations\\\",\\n            #                     \\\"extent\\\": [min(dim), max(dim)],\\n            #                     \\\"unit\\\": \\\"-\\\",\\n            #                 }\\n            #             else:\\n            dimdict = {\\n                \\\"type\\\": \\\"temporal\\\",  # TODO To be customized based on variable?\\n                \\\"values\\\": dim[:].values.tolist(),\\n                \\\"unit\\\": dim.attrs.get(\\\"units\\\", \\\"-\\\"),\\n            }\\n            dim = Dimension.from_dict(dimdict)\\n            cube_dimensions[var] = dim\\n\\ndimvals = {k: v.values for k, v in cube_dimensions.items() if v.values}\\n\\n# dot product of variables keys\\nkeys = []\\nfor values in product(*dimvals.values()):\\n    # TODO Improve key gen and align with geojson generation\\n    keys.append(\\n        \\\"-\\\".join(map(lambda x: \\\"-\\\".join(x), zip(dimvals.keys(), map(str, values))))\\n    )\\n\\n# flatten single data values over specific dimension keys\\nfeatures = []\\nfor j, (lon, lat) in enumerate(\\n    zip(nddata[\\\"longitude\\\"][:].values, nddata[\\\"latitude\\\"][:].values)\\n):  # assumes longitude and latitude are present and independent\\n    point = geojson.Point((float(lon), float(lat)))\\n    feature = geojson.Feature(geometry=point)\\n    feature[\\\"properties\\\"][\\\"locationId\\\"] = j\\n\\n    for a, b in zip(\\n        nddata[\\\"%s\\\" % variable][:, j, :].values.flatten(), keys\\n    ):  # flattened along dimensions\\n        feature[\\\"properties\\\"][b] = a\\n\\n    features.append(feature)\\n\\n# store the features in a GeoJSON file\\ncollection = geojson.FeatureCollection(features)\\nwith open(\\n    os.path.join(dataset_dir, \\\"platform\\\", r\\\"%s.geojson\\\" % dataset_out_file), \\\"w\\\"\\n) as f:\\n    geojson.dump(collection, f)\";\n",
       "                var nbb_formatted_code = \"# write geoJSON\\n\\n# automated variable retrieval (without hidden files)\\nvariables = list(nddata.variables)\\n\\n# autmated dimension retrieval\\ndimensions = list(nddata[\\\"%s\\\" % (variable)].dims)\\n\\n# write data to flattened GeoJSON file - Mapbox styling uses this (aligned with STAC)\\ncube_dimensions = {}\\nfor dimension in dimensions:  # loop over dimensions of variable\\n\\n    for var in variables:  # identify variable with dimension\\n        var_dim = nddata[\\\"%s\\\" % var].dims\\n        if (\\n            len(var_dim) == 1 and var_dim[0] == dimension and dimension != \\\"stations\\\"\\n        ):  # only take the variables in the coordinates with a single dimension, where dimension is equal to the\\n            # variable dimensions and where it is not equal to stations (assumed present and independent)\\n\\n            dim = nddata[var]\\n\\n            # Only applicable in proper CF convention zarr\\n            #             if dimension == \\\"stations\\\": # TODO Remove because this is abundant due to if statement\\n            #                 dimdict = {\\n            #                     \\\"type\\\": \\\"stations\\\",\\n            #                     \\\"extent\\\": [min(dim), max(dim)],\\n            #                     \\\"unit\\\": \\\"-\\\",\\n            #                 }\\n            #             else:\\n            dimdict = {\\n                \\\"type\\\": \\\"temporal\\\",  # TODO To be customized based on variable?\\n                \\\"values\\\": dim[:].values.tolist(),\\n                \\\"unit\\\": dim.attrs.get(\\\"units\\\", \\\"-\\\"),\\n            }\\n            dim = Dimension.from_dict(dimdict)\\n            cube_dimensions[var] = dim\\n\\ndimvals = {k: v.values for k, v in cube_dimensions.items() if v.values}\\n\\n# dot product of variables keys\\nkeys = []\\nfor values in product(*dimvals.values()):\\n    # TODO Improve key gen and align with geojson generation\\n    keys.append(\\n        \\\"-\\\".join(map(lambda x: \\\"-\\\".join(x), zip(dimvals.keys(), map(str, values))))\\n    )\\n\\n# flatten single data values over specific dimension keys\\nfeatures = []\\nfor j, (lon, lat) in enumerate(\\n    zip(nddata[\\\"longitude\\\"][:].values, nddata[\\\"latitude\\\"][:].values)\\n):  # assumes longitude and latitude are present and independent\\n    point = geojson.Point((float(lon), float(lat)))\\n    feature = geojson.Feature(geometry=point)\\n    feature[\\\"properties\\\"][\\\"locationId\\\"] = j\\n\\n    for a, b in zip(\\n        nddata[\\\"%s\\\" % variable][:, j, :].values.flatten(), keys\\n    ):  # flattened along dimensions\\n        feature[\\\"properties\\\"][b] = a\\n\\n    features.append(feature)\\n\\n# store the features in a GeoJSON file\\ncollection = geojson.FeatureCollection(features)\\nwith open(\\n    os.path.join(dataset_dir, \\\"platform\\\", r\\\"%s.geojson\\\" % dataset_out_file), \\\"w\\\"\\n) as f:\\n    geojson.dump(collection, f)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write geoJSON\n",
    "\n",
    "# automated variable retrieval (without hidden files)\n",
    "variables = list(nddata.variables)\n",
    "\n",
    "# autmated dimension retrieval\n",
    "dimensions = list(nddata[\"%s\" % (variable)].dims)\n",
    "\n",
    "# write data to flattened GeoJSON file - Mapbox styling uses this (aligned with STAC)\n",
    "cube_dimensions = {}\n",
    "for dimension in dimensions:  # loop over dimensions of variable\n",
    "\n",
    "    for var in variables:  # identify variable with dimension\n",
    "        var_dim = nddata[\"%s\" % var].dims\n",
    "        if (\n",
    "            len(var_dim) == 1 and var_dim[0] == dimension and dimension != \"stations\"\n",
    "        ):  # only take the variables in the coordinates with a single dimension, where dimension is equal to the\n",
    "            # variable dimensions and where it is not equal to stations (assumed present and independent)\n",
    "\n",
    "            dim = nddata[var]\n",
    "\n",
    "            # Only applicable in proper CF convention zarr\n",
    "            #             if dimension == \"stations\": # TODO Remove because this is abundant due to if statement\n",
    "            #                 dimdict = {\n",
    "            #                     \"type\": \"stations\",\n",
    "            #                     \"extent\": [min(dim), max(dim)],\n",
    "            #                     \"unit\": \"-\",\n",
    "            #                 }\n",
    "            #             else:\n",
    "            dimdict = {\n",
    "                \"type\": \"temporal\",  # TODO To be customized based on variable?\n",
    "                \"values\": dim[:].values.tolist(),\n",
    "                \"unit\": dim.attrs.get(\"units\", \"-\"),\n",
    "            }\n",
    "            dim = Dimension.from_dict(dimdict)\n",
    "            cube_dimensions[var] = dim\n",
    "\n",
    "dimvals = {k: v.values for k, v in cube_dimensions.items() if v.values}\n",
    "\n",
    "# dot product of variables keys\n",
    "keys = []\n",
    "for values in product(*dimvals.values()):\n",
    "    # TODO Improve key gen and align with geojson generation\n",
    "    keys.append(\n",
    "        \"-\".join(map(lambda x: \"-\".join(x), zip(dimvals.keys(), map(str, values))))\n",
    "    )\n",
    "\n",
    "# flatten single data values over specific dimension keys\n",
    "features = []\n",
    "for j, (lon, lat) in enumerate(\n",
    "    zip(nddata[\"longitude\"][:].values, nddata[\"latitude\"][:].values)\n",
    "):  # assumes longitude and latitude are present and independent\n",
    "    point = geojson.Point((float(lon), float(lat)))\n",
    "    feature = geojson.Feature(geometry=point)\n",
    "    feature[\"properties\"][\"locationId\"] = j\n",
    "\n",
    "    for a, b in zip(\n",
    "        nddata[\"%s\" % variable][:, j, :].values.flatten(), keys\n",
    "    ):  # flattened along dimensions\n",
    "        feature[\"properties\"][b] = a\n",
    "\n",
    "    features.append(feature)\n",
    "\n",
    "# store the features in a GeoJSON file\n",
    "collection = geojson.FeatureCollection(features)\n",
    "with open(\n",
    "    os.path.join(dataset_dir, \"platform\", r\"%s.geojson\" % dataset_out_file), \"w\"\n",
    ") as f:\n",
    "    geojson.dump(collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729c8858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"geometry\": {\"coordinates\": [-0.1, 49.7], \"type\": \"Point\"}, \"properties\": {\"locationId\": 2, \"scenario-historical-RP-10.0\": 2.08892, \"scenario-historical-RP-100.0\": 2.43342, \"scenario-historical-RP-1000.0\": 2.66859, \"scenario-historical-RP-20.0\": 2.20901, \"scenario-historical-RP-200.0\": 2.51257, \"scenario-historical-RP-5.0\": 1.95, \"scenario-historical-RP-50.0\": 2.3447, \"scenario-historical-RP-500.0\": 2.60544, \"scenario-rcp45-RP-10.0\": 2.12185, \"scenario-rcp45-RP-100.0\": 2.49559, \"scenario-rcp45-RP-1000.0\": 2.75567, \"scenario-rcp45-RP-20.0\": 2.25063, \"scenario-rcp45-RP-200.0\": 2.58303, \"scenario-rcp45-RP-5.0\": 1.97503, \"scenario-rcp45-RP-50.0\": 2.39813, \"scenario-rcp45-RP-500.0\": 2.68587, \"scenario-rcp85-RP-10.0\": 2.12493, \"scenario-rcp85-RP-100.0\": 2.44322, \"scenario-rcp85-RP-1000.0\": 2.62478, \"scenario-rcp85-RP-20.0\": 2.24118, \"scenario-rcp85-RP-200.0\": 2.50843, \"scenario-rcp85-RP-5.0\": 1.985, \"scenario-rcp85-RP-50.0\": 2.36607, \"scenario-rcp85-RP-500.0\": 2.57984}, \"type\": \"Feature\"}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# check written geojson\\n\\n# check shape\\nwith open(os.path.join(dataset_dir, \\\"platform\\\", \\\"%s.geojson\\\" % dataset_out_file)) as f:\\n    check = geojson.load(f)\\n\\nprint(check[\\\"features\\\"][2])\\n\\n# check the minima and maxima for the colormap boundaries\\n# scenario = 0\\n# for idx, i in enumerate(dataset[\\\"RP\\\"][:].values):\\n#     print(\\n#         i,\\n#         round(min(nddata[\\\"ssl\\\"][scenario, :, idx].values), 2),\\n#         round(max(nddata[\\\"ssl\\\"][scenario, :, idx].values), 2),\\n#     )\";\n",
       "                var nbb_formatted_code = \"# check written geojson\\n\\n# check shape\\nwith open(os.path.join(dataset_dir, \\\"platform\\\", \\\"%s.geojson\\\" % dataset_out_file)) as f:\\n    check = geojson.load(f)\\n\\nprint(check[\\\"features\\\"][2])\\n\\n# check the minima and maxima for the colormap boundaries\\n# scenario = 0\\n# for idx, i in enumerate(dataset[\\\"RP\\\"][:].values):\\n#     print(\\n#         i,\\n#         round(min(nddata[\\\"ssl\\\"][scenario, :, idx].values), 2),\\n#         round(max(nddata[\\\"ssl\\\"][scenario, :, idx].values), 2),\\n#     )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check written geojson\n",
    "\n",
    "# check shape\n",
    "with open(os.path.join(dataset_dir, \"platform\", \"%s.geojson\" % dataset_out_file)) as f:\n",
    "    check = geojson.load(f)\n",
    "\n",
    "print(check[\"features\"][2])\n",
    "\n",
    "# check the minima and maxima for the colormap boundaries\n",
    "# scenario = 0\n",
    "# for idx, i in enumerate(dataset[\"RP\"][:].values):\n",
    "#     print(\n",
    "#         i,\n",
    "#         round(min(nddata[\"ssl\"][scenario, :, idx].values), 2),\n",
    "#         round(max(nddata[\"ssl\"][scenario, :, idx].values), 2),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2ccc1",
   "metadata": {},
   "source": [
    "# 3. upload Zarr to GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f94af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder uploaded to GCS\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# upload zarr folder to GCS\\nos.environ[\\\"GOOGLE_APPLICATION_CREDENTIALS\\\"] = str(GCS_token)\\n\\n# function to upload zarr folder to GCS\\nstorage_client = storage.Client()\\n\\n\\ndef upload_from_directory(directory_path, dest_bucket_name, dest_blob_name):\\n    rel_paths = directory_path.glob(\\\"**/*\\\")\\n    bucket = storage_client.bucket(dest_bucket_name)\\n    for local_file in rel_paths:\\n        remote_path = f'{dest_blob_name}/{\\\"/\\\".join(str(local_file).split(os.sep)[5:])}'  # note 5: is hardcoded and might lead to problems\\n        if os.path.isfile(local_file):\\n            blob = bucket.blob(remote_path)\\n            blob.upload_from_filename(local_file)\\n\\n    # print status\\n    print(\\\"Folder uploaded to GCS\\\")\\n\\n\\n# specification of directory, bucket and file name to feed into the function\\ndirectory_path = dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file)\\ndest_bucket_name = \\\"dgds-data-public\\\"\\ndest_blob_name = \\\"coclico/\\\" + dataset_out_file + \\\".zarr\\\"\\nfolder_upload = upload_from_directory(directory_path, dest_bucket_name, dest_blob_name)\";\n",
       "                var nbb_formatted_code = \"# upload zarr folder to GCS\\nos.environ[\\\"GOOGLE_APPLICATION_CREDENTIALS\\\"] = str(GCS_token)\\n\\n# function to upload zarr folder to GCS\\nstorage_client = storage.Client()\\n\\n\\ndef upload_from_directory(directory_path, dest_bucket_name, dest_blob_name):\\n    rel_paths = directory_path.glob(\\\"**/*\\\")\\n    bucket = storage_client.bucket(dest_bucket_name)\\n    for local_file in rel_paths:\\n        remote_path = f'{dest_blob_name}/{\\\"/\\\".join(str(local_file).split(os.sep)[5:])}'  # note 5: is hardcoded and might lead to problems\\n        if os.path.isfile(local_file):\\n            blob = bucket.blob(remote_path)\\n            blob.upload_from_filename(local_file)\\n\\n    # print status\\n    print(\\\"Folder uploaded to GCS\\\")\\n\\n\\n# specification of directory, bucket and file name to feed into the function\\ndirectory_path = dataset_dir.joinpath(\\\"%s.zarr\\\" % dataset_out_file)\\ndest_bucket_name = \\\"dgds-data-public\\\"\\ndest_blob_name = \\\"coclico/\\\" + dataset_out_file + \\\".zarr\\\"\\nfolder_upload = upload_from_directory(directory_path, dest_bucket_name, dest_blob_name)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload zarr folder to GCS\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(GCS_token)\n",
    "\n",
    "# function to upload zarr folder to GCS\n",
    "storage_client = storage.Client()\n",
    "\n",
    "\n",
    "def upload_from_directory(directory_path, dest_bucket_name, dest_blob_name):\n",
    "    rel_paths = directory_path.glob(\"**/*\")\n",
    "    bucket = storage_client.bucket(dest_bucket_name)\n",
    "    for local_file in rel_paths:\n",
    "        remote_path = f'{dest_blob_name}/{\"/\".join(str(local_file).split(os.sep)[5:])}'  # note 5: is hardcoded and might lead to problems\n",
    "        if os.path.isfile(local_file):\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "\n",
    "    # print status\n",
    "    print(\"Folder uploaded to GCS\")\n",
    "\n",
    "\n",
    "# specification of directory, bucket and file name to feed into the function\n",
    "directory_path = dataset_dir.joinpath(\"%s.zarr\" % dataset_out_file)\n",
    "dest_bucket_name = \"dgds-data-public\"\n",
    "dest_blob_name = \"coclico/\" + dataset_out_file + \".zarr\"\n",
    "folder_upload = upload_from_directory(directory_path, dest_bucket_name, dest_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2b32e",
   "metadata": {},
   "source": [
    "# 4. upload geoJSON to Mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59168879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['mapbox', '--access-token', 'sk.eyJ1IjoiZ2xvYmFsLWRhdGEtdmlld2VyIiwiYSI6ImNsMnB0dmlscTFrZnEzY211cWxna3Bxb3AifQ.OQ3P9PEjZUiErvjrwVbsag', 'upload', 'global-data-viewer.CoastAlRisk_Europe_EESSL', 'P:\\\\11205479-coclico\\\\data\\\\01_storm_surge_jrc\\\\platform\\\\CoastAlRisk_Europe_EESSL.geojson'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# ingest geoJSON into mapbox tilesets\\n\\n# python way of running CLI to upload to mapbox automatically\\nsubprocess.run(\\n    [\\n        \\\"mapbox\\\",\\n        \\\"--access-token\\\",\\n        mapbox_token,\\n        \\\"upload\\\",\\n        r\\\"global-data-viewer.%s\\\" % dataset_out_file.split(\\\".\\\")[0],\\n        os.path.join(\\n            dataset_dir, \\\"platform\\\", r\\\"%s.geojson\\\" % dataset_out_file.split(\\\".\\\")[0]\\n        ),\\n    ],\\n    shell=True,\\n    check=True,\\n)\\n\\n# notebook version of CLI\\n#!mapbox --access-token {mapbox_token} upload {filename} {source}\\n\\n# CLI command (example)\\n# mapbox --access-token **write out mapbox_token** upload global-data-viewer.CoastAlRisk_Europe_EESSL p:\\\\11205479-coclico\\\\data\\\\01_storm_surge_jrc\\\\platform\\\\CoastAlRisk_Europe_EESSL.geojson\";\n",
       "                var nbb_formatted_code = \"# ingest geoJSON into mapbox tilesets\\n\\n# python way of running CLI to upload to mapbox automatically\\nsubprocess.run(\\n    [\\n        \\\"mapbox\\\",\\n        \\\"--access-token\\\",\\n        mapbox_token,\\n        \\\"upload\\\",\\n        r\\\"global-data-viewer.%s\\\" % dataset_out_file.split(\\\".\\\")[0],\\n        os.path.join(\\n            dataset_dir, \\\"platform\\\", r\\\"%s.geojson\\\" % dataset_out_file.split(\\\".\\\")[0]\\n        ),\\n    ],\\n    shell=True,\\n    check=True,\\n)\\n\\n# notebook version of CLI\\n#!mapbox --access-token {mapbox_token} upload {filename} {source}\\n\\n# CLI command (example)\\n# mapbox --access-token **write out mapbox_token** upload global-data-viewer.CoastAlRisk_Europe_EESSL p:\\\\11205479-coclico\\\\data\\\\01_storm_surge_jrc\\\\platform\\\\CoastAlRisk_Europe_EESSL.geojson\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ingest geoJSON into mapbox tilesets\n",
    "\n",
    "# python way of running CLI to upload to mapbox automatically\n",
    "subprocess.run(\n",
    "    [\n",
    "        \"mapbox\",\n",
    "        \"--access-token\",\n",
    "        mapbox_token,\n",
    "        \"upload\",\n",
    "        r\"global-data-viewer.%s\" % dataset_out_file.split(\".\")[0],\n",
    "        os.path.join(\n",
    "            dataset_dir, \"platform\", r\"%s.geojson\" % dataset_out_file.split(\".\")[0]\n",
    "        ),\n",
    "    ],\n",
    "    shell=True,\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "# notebook version of CLI\n",
    "#!mapbox --access-token {mapbox_token} upload {filename} {source}\n",
    "\n",
    "# CLI command (example)\n",
    "# mapbox --access-token **write out mapbox_token** upload global-data-viewer.CoastAlRisk_Europe_EESSL p:\\11205479-coclico\\data\\01_storm_surge_jrc\\platform\\CoastAlRisk_Europe_EESSL.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e5bbd",
   "metadata": {},
   "source": [
    "# 5. Update the STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04f8c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# update STAC\\n\\n# Get initial STAC\\ncollection = Collection.from_file(os.path.join(local_STAC, \\\"current/collection.json\\\"))\\n# collection.describe()  # display hierarchy\\n\\n# Get template and set items\\ntemplatedataset = collection.get_child(template)\\ndataset = templatedataset.full_copy()\\ndataset.id = datasetid\\ndataset.title = variable\\ndataset.description = variable\\n\\n# Drop existing items, dimensions and summaries\\ndataset._resolved_objects\\ndataset.set_root(None)\\ndataset.clear_items()\\ndataset.assets = {}\\ndataset.extra_fields = deepcopy(\\n    dataset.extra_fields\\n)  # workaround for https://github.com/stac-utils/pystac/issues/787\\ndataset.summaries = None\\ndataset.extra_fields.pop(\\\"cube:dimensions\\\", None)\\ndataset.extra_fields.pop(\\\"cube:variables\\\", None)\\ndataset.extra_fields.pop(\\\"summaries\\\", None)\\n\\n# Add zarr asset\\ndataset.add_asset(\\\"data\\\", generate.gen_zarr_asset(variable, zarr_fn))\\n\\n# Add dimension info\\ndc_ext = DatacubeExtension.ext(dataset)\\ndc_ext.apply(cube_dimensions)\\n\\nvar = Variable({})\\nvar.description = \\\"\\\"\\nvar.dimensions = list(cube_dimensions.keys())\\nvar.type = \\\"data\\\"\\nvar.unit = nddata[\\\"%s\\\" % variable].attrs[\\\"units\\\"]\\ndc_ext.variables = {variable: var}\\n\\n# Add summaries\\ndataset.summaries = Summaries(summaries=dimvals)\\n\\n# Add children\\nlayout = generate.Layout()\\nfor values in product(*dimvals.values()):\\n    # TODO Improve key gen and align with geojson generation\\n    key = \\\"-\\\".join(map(lambda x: \\\"-\\\".join(x), zip(dimvals.keys(), map(str, values))))\\n    feature = generate.gen_default_item(f\\\"{variable}-mapbox-{key}\\\")\\n    feature.add_asset(\\\"mapbox\\\", generate.gen_mapbox_asset(mapbox_url, dataset_out_file))\\n    feature.properties = generate.gen_default_props(key=key)\\n    for (k, v) in zip(dimvals.keys(), values):\\n        feature.properties[k] = v\\n    dataset.add_item(feature, strategy=layout)\\n\\n\\n# Set extra link properties\\ngenerate.extend_links(dataset, cube_dimensions.keys())\\n\\n# Save and limit number of folders\\ncollection.add_child(dataset)\\ndataset.normalize_hrefs(\\n    os.path.join(local_STAC, f\\\"current/{variable}\\\"), strategy=layout\\n)\\ncollection.save(\\n    catalog_type=CatalogType.SELF_CONTAINED,\\n    dest_href=os.path.join(local_STAC, f\\\"current\\\"),\\n    stac_io=generate.IO(),\\n)\";\n",
       "                var nbb_formatted_code = \"# update STAC\\n\\n# Get initial STAC\\ncollection = Collection.from_file(os.path.join(local_STAC, \\\"current/collection.json\\\"))\\n# collection.describe()  # display hierarchy\\n\\n# Get template and set items\\ntemplatedataset = collection.get_child(template)\\ndataset = templatedataset.full_copy()\\ndataset.id = datasetid\\ndataset.title = variable\\ndataset.description = variable\\n\\n# Drop existing items, dimensions and summaries\\ndataset._resolved_objects\\ndataset.set_root(None)\\ndataset.clear_items()\\ndataset.assets = {}\\ndataset.extra_fields = deepcopy(\\n    dataset.extra_fields\\n)  # workaround for https://github.com/stac-utils/pystac/issues/787\\ndataset.summaries = None\\ndataset.extra_fields.pop(\\\"cube:dimensions\\\", None)\\ndataset.extra_fields.pop(\\\"cube:variables\\\", None)\\ndataset.extra_fields.pop(\\\"summaries\\\", None)\\n\\n# Add zarr asset\\ndataset.add_asset(\\\"data\\\", generate.gen_zarr_asset(variable, zarr_fn))\\n\\n# Add dimension info\\ndc_ext = DatacubeExtension.ext(dataset)\\ndc_ext.apply(cube_dimensions)\\n\\nvar = Variable({})\\nvar.description = \\\"\\\"\\nvar.dimensions = list(cube_dimensions.keys())\\nvar.type = \\\"data\\\"\\nvar.unit = nddata[\\\"%s\\\" % variable].attrs[\\\"units\\\"]\\ndc_ext.variables = {variable: var}\\n\\n# Add summaries\\ndataset.summaries = Summaries(summaries=dimvals)\\n\\n# Add children\\nlayout = generate.Layout()\\nfor values in product(*dimvals.values()):\\n    # TODO Improve key gen and align with geojson generation\\n    key = \\\"-\\\".join(map(lambda x: \\\"-\\\".join(x), zip(dimvals.keys(), map(str, values))))\\n    feature = generate.gen_default_item(f\\\"{variable}-mapbox-{key}\\\")\\n    feature.add_asset(\\\"mapbox\\\", generate.gen_mapbox_asset(mapbox_url, dataset_out_file))\\n    feature.properties = generate.gen_default_props(key=key)\\n    for (k, v) in zip(dimvals.keys(), values):\\n        feature.properties[k] = v\\n    dataset.add_item(feature, strategy=layout)\\n\\n\\n# Set extra link properties\\ngenerate.extend_links(dataset, cube_dimensions.keys())\\n\\n# Save and limit number of folders\\ncollection.add_child(dataset)\\ndataset.normalize_hrefs(\\n    os.path.join(local_STAC, f\\\"current/{variable}\\\"), strategy=layout\\n)\\ncollection.save(\\n    catalog_type=CatalogType.SELF_CONTAINED,\\n    dest_href=os.path.join(local_STAC, f\\\"current\\\"),\\n    stac_io=generate.IO(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# update STAC\n",
    "\n",
    "# Get initial STAC\n",
    "collection = Collection.from_file(os.path.join(local_STAC, \"current/collection.json\"))\n",
    "# collection.describe()  # display hierarchy\n",
    "\n",
    "# Get template and set items\n",
    "templatedataset = collection.get_child(template)\n",
    "dataset = templatedataset.full_copy()\n",
    "dataset.id = datasetid\n",
    "dataset.title = variable\n",
    "dataset.description = variable\n",
    "\n",
    "# Drop existing items, dimensions and summaries\n",
    "dataset._resolved_objects\n",
    "dataset.set_root(None)\n",
    "dataset.clear_items()\n",
    "dataset.assets = {}\n",
    "dataset.extra_fields = deepcopy(\n",
    "    dataset.extra_fields\n",
    ")  # workaround for https://github.com/stac-utils/pystac/issues/787\n",
    "dataset.summaries = None\n",
    "dataset.extra_fields.pop(\"cube:dimensions\", None)\n",
    "dataset.extra_fields.pop(\"cube:variables\", None)\n",
    "dataset.extra_fields.pop(\"summaries\", None)\n",
    "\n",
    "# Add zarr asset\n",
    "dataset.add_asset(\"data\", generate.gen_zarr_asset(variable, zarr_fn))\n",
    "\n",
    "# Add dimension info\n",
    "dc_ext = DatacubeExtension.ext(dataset)\n",
    "dc_ext.apply(cube_dimensions)\n",
    "\n",
    "var = Variable({})\n",
    "var.description = \"\"\n",
    "var.dimensions = list(cube_dimensions.keys())\n",
    "var.type = \"data\"\n",
    "var.unit = nddata[\"%s\" % variable].attrs[\"units\"]\n",
    "dc_ext.variables = {variable: var}\n",
    "\n",
    "# Add summaries\n",
    "dataset.summaries = Summaries(summaries=dimvals)\n",
    "\n",
    "# Add children\n",
    "layout = generate.Layout()\n",
    "for values in product(*dimvals.values()):\n",
    "    # TODO Improve key gen and align with geojson generation\n",
    "    key = \"-\".join(map(lambda x: \"-\".join(x), zip(dimvals.keys(), map(str, values))))\n",
    "    feature = generate.gen_default_item(f\"{variable}-mapbox-{key}\")\n",
    "    feature.add_asset(\"mapbox\", generate.gen_mapbox_asset(mapbox_url, dataset_out_file))\n",
    "    feature.properties = generate.gen_default_props(key=key)\n",
    "    for (k, v) in zip(dimvals.keys(), values):\n",
    "        feature.properties[k] = v\n",
    "    dataset.add_item(feature, strategy=layout)\n",
    "\n",
    "\n",
    "# Set extra link properties\n",
    "generate.extend_links(dataset, cube_dimensions.keys())\n",
    "\n",
    "# Save and limit number of folders\n",
    "collection.add_child(dataset)\n",
    "dataset.normalize_hrefs(\n",
    "    os.path.join(local_STAC, f\"current/{variable}\"), strategy=layout\n",
    ")\n",
    "collection.save(\n",
    "    catalog_type=CatalogType.SELF_CONTAINED,\n",
    "    dest_href=os.path.join(local_STAC, f\"current\"),\n",
    "    stac_io=generate.IO(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19354d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
